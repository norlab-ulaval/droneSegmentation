{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gsd_utils import evaluate_segmentation\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "from pycm import ConfusionMatrix\n",
    "import json\n",
    "import cv2\n",
    "# plt.rcParams.update({'font.size': 4})\n",
    "\n",
    "data_path = Path(\"/data/M2F_pred_finetuning\")\n",
    "pred_path = data_path / \"pred/output_test\"\n",
    "true_path = data_path / \"target/Test_Annotated_masks\"\n",
    "\n",
    "preds_dir = Path(\"predictions\")\n",
    "m2fpreds_dir = preds_dir / \"M2F-FT\"\n",
    "m2fpreds_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "confmat_dir = preds_dir / \"confmat\"\n",
    "confmat_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ious, accs, f1s, all_predictions, all_annotations = evaluate_segmentation(\n",
    "    pred_path,\n",
    "    true_path,\n",
    "    ignored_classes=[1],\n",
    "    num_classes=26,\n",
    ")\n",
    "print(all_predictions.shape, all_annotations.shape)\n",
    "np.save(m2fpreds_dir / \"predictions.npy\", all_predictions)\n",
    "np.save(m2fpreds_dir / \"annotations.npy\", all_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = json.loads(Path(\"label_to_id.json\").read_bytes())\n",
    "id_class = {y: x for x, y in class_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_paths = [p for p in pred_path.rglob(\"*.png\")]\n",
    "file_map = {pred: next(true_path.rglob(f\"{pred.stem}*\")) for pred in pred_paths}\n",
    "\n",
    "pred_arrs = [cv2.imread(p, cv2.IMREAD_GRAYSCALE) for p in file_map.keys()]\n",
    "true_arrs = [cv2.imread(file_map[p], cv2.IMREAD_GRAYSCALE) for p in file_map.keys()]\n",
    "\n",
    "pred_values = np.vstack([im.flatten() for im in pred_arrs]).flatten()\n",
    "true_values = np.vstack([im.flatten() for im in true_arrs]).flatten()\n",
    "\n",
    "pred_labels = np.vectorize(id_class.get)(pred_values)\n",
    "true_labels = np.vectorize(id_class.get)(true_values)\n",
    "\n",
    "all_labels = np.vectorize(id_class.get)(np.arange(len(id_class)))\n",
    "\n",
    "pred_labels.shape, true_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_values = np.stack([pred_values, true_values])\n",
    "all_uniques = np.unique(all_model_values)\n",
    "present_labels = [id_class[idx] for idx in all_uniques]\n",
    "inconsistent = [v for v in np.arange(len(id_class)) if v not in all_uniques]\n",
    "inconsistent, [id_class[cl] for cl in inconsistent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = {\"target\": true_values, \"pred\": pred_values}\n",
    "precs = precision_score(classification[\"target\"], classification[\"pred\"], average=None)\n",
    "f1s = f1_score(classification[\"target\"], classification[\"pred\"], average=None)\n",
    "recs = recall_score(classification[\"target\"], classification[\"pred\"], average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(\n",
    "    {\"name\": present_labels, \"f1\": f1s, \"precision\": precs, \"recall\": recs}\n",
    ")\n",
    "metrics.precision *= 100\n",
    "metrics.f1 *= 100\n",
    "metrics.recall *= 100\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.to_csv(m2fpreds_dir / \"metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = {\"target\": all_annotations, \"pred\": all_predictions}\n",
    "precs = precision_score(classification[\"target\"], classification[\"pred\"], average=None)\n",
    "f1s = f1_score(classification[\"target\"], classification[\"pred\"], average=None)\n",
    "recs = recall_score(classification[\"target\"], classification[\"pred\"], average=None)\n",
    "class_metrics = pd.DataFrame(\n",
    "    {\"name\": present_labels, \"f1\": f1s, \"precision\": precs, \"recall\": recs}\n",
    ")\n",
    "class_metrics.precision *= 100\n",
    "class_metrics.f1 *= 100\n",
    "class_metrics.recall *= 100\n",
    "class_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(all_annotations, all_predictions, is_imbalanced=True)\n",
    "cm.plot(normalized=True, cmap=\"Blues\")\n",
    "cm.save_html(str(confmat_dir / \"them\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(\n",
    "    actual_vector=true_labels,\n",
    "    predict_vector=pred_labels,\n",
    "    is_imbalanced=True,\n",
    ")\n",
    "cm.plot(normalized=True, cmap=\"Blues\", number_label=False)\n",
    "cm.save_html(str(confmat_dir / \"labelled\"), color=\"Blues\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "disp = ConfusionMatrixDisplay.from_predictions(\n",
    "    all_annotations,\n",
    "    all_predictions,\n",
    "    cmap=\"Blues\",\n",
    "    normalize=\"true\",\n",
    "    values_format=\"2%\",\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(true_values, pred_values, normalize=\"true\")\n",
    "cmp = ConfusionMatrixDisplay(cm, display_labels=[id_class[cl] for cl in all_uniques])\n",
    "fig, ax = plt.subplots(figsize=(30, 10))\n",
    "cmp.plot(\n",
    "    ax=ax,\n",
    "    colorbar=False,\n",
    "    cmap=\"Blues\",\n",
    "    values_format=\"2.0%\",\n",
    "    include_values=False,\n",
    ")\n",
    "cax = fig.add_axes(\n",
    "    [ax.get_position().x1 + 0.01, ax.get_position().y0, 0.02, ax.get_position().height]\n",
    ")\n",
    "plt.colorbar(cmp.im_, cax=cax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "disp = ConfusionMatrixDisplay.from_predictions(\n",
    "    true_labels,\n",
    "    pred_labels,\n",
    "    cmap=\"Blues\",\n",
    "    normalize=\"true\",\n",
    "    values_format=\"%\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {\n",
    "    \"them\": {\"target\": all_annotations, \"pred\": all_predictions},\n",
    "    \"ours\": {\"target\": true_values, \"pred\": pred_values},\n",
    "}\n",
    "# Precision\n",
    "for k, v in values.items():\n",
    "    print(\n",
    "        k,\n",
    "        precision_score(v[\"target\"], v[\"pred\"], average=\"weighted\"),\n",
    "        recall_score(v[\"target\"], v[\"pred\"], average=\"weighted\"),\n",
    "        f1_score(v[\"target\"], v[\"pred\"], average=\"weighted\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(\n",
    "    classification[\"target\"],\n",
    "    classification[\"pred\"],\n",
    "    average=None,\n",
    "    labels=[str(a) for a in np.arange(24)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(true_values, pred_values)\n",
    "cmp = ConfusionMatrixDisplay(cm, display_labels=all_labels)\n",
    "cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
